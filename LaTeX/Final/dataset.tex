\section{Dataset and Features}
\label{sec:dataset}

One key feature of our project is that it does not require a large corpus of training examples; rather, every execution of our algorithm takes only one color image and one grayscale image as input. We used publicly available pictures with a typical resolution of $600\times400$ pixels, although there is no constraint on the size of our input.  Because our algorithm will ultimately be applied to grayscale testing images, the features we extract and learn upon must all be derivable from this type of image.  In particular, none of the features can be based on information from the color channels of the training input. Thus, we attempt to match regions with similar content in both images based on the similarity of the underlying textures in the luminance channel. In our implementation, we use SURF and FFT descriptors to extract information about textures. 

SURF stands for \emph{Speeded Up Robust Features} and was first introduced by H. Bay et al. in 2006 \cite{bay2008speeded}. First, it uses a blob detector based on an approximation of the Hessian matrix to locate changes in the image's texture. These changes are associated with interest points, which identify the distinctive patters of the picture. For every interest point, horizontal and vertical Haar wavelet responses are computed in the sub-regions of a square centered at that point. These responses are then weighted and reduced to a lower dimensional space to produce a 128-dimensional local descriptor. One important criterion for choosing SURF is its invariance under geometrical transformations, like scaling and rotation. Thus the descriptor is consistent across different parts of the picture and even across different pictures. Moreover, computing SURF features is relatively fast, which allows us to apply it to all the pixels.

We also compute the Fast Fourier Transform (FFT) as part of our feature vector. FFT represents the image as a sum of exponentials of different magnitudes and frequencies and can therefore be used to gain information about the frequency domain of our image. It is also useful for finding correlations between images because the convolution function is similar to the correlation function \cite{gonzalez2002woods}. 

For every pixel $p$ in the training and testing images, we consider a $15\times15$ pixel window centered at $p$. The dimension of the window was chosen so that it captures enough information about the neighborhoods of $p$ but doesn't slow down our implementation. By applying SURF to this window and two blurred version of it, we extract 3 SURF vectors, which we then concatenate to form a 384-dimensional feature vector. To that vector we add the 225 FFT features as well as the mean and standard deviation of the luminance around $p$. We end up with a 611-dimensional feature vector, which we further reduce to only $30$ dimensions by applying PCA.  Although the dimensionality of the data has been significantly reduced, we found that these top 30 principle components retain around 70\% of the variance in the data, meaning that we are still able to richly describe the image textures using this feature space.